{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76425e-d52a-49ac-98ae-3c595f4ac5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.52, NSE: 0.99\n",
      "Ensemble 0 done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "import os\n",
    "os.chdir(\"/work/pi_kandread_umass_edu/lake_temp_bias/satbias_model/satlswt\")\n",
    "\n",
    "# ========================================= Training characteristics ==============================================\n",
    "# read the job\n",
    "job = 1\n",
    "# hydrolake depth\n",
    "hydrolake = pd.read_csv(\"data/cci_lakes_hydrolake_depth.csv\", index_col = 0)\n",
    "# list of cci lakes. Some lakes don't have ERA5-Land data, which are excluded\n",
    "cci_lake_list = hydrolake.index.to_numpy().astype(np.int64)\n",
    "# cci lake id\n",
    "lake_id = str(cci_lake_list[job])\n",
    "\n",
    "\n",
    "lake_id = str(733)\n",
    "\n",
    "# file directory to save parameter\n",
    "param_dir = f\"/work/pi_kandread_umass_edu/lake_temp_bias/satbias_model/satlswt/params/lstm_param_full\"\n",
    "# data path\n",
    "air_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/air_temp.csv\"\n",
    "wind_path = \"/nas/cee-hydro/laketemp_bias/era5land/wind.csv\"\n",
    "srad_path = \"/nas/cee-hydro/laketemp_bias/era5land/srad.csv\"\n",
    "water_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/water_temp.csv\"\n",
    "# ensemble\n",
    "ensemble_num = 10\n",
    "# use cpu or gpu\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# train test period\n",
    "train_period = (pd.to_datetime(\"2003-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2017-12-31\", format=\"%Y-%m-%d\"))\n",
    "val_period = (pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\"))\n",
    "# test_period = (pd.to_datetime(\"2020-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\"))\n",
    "# hyperparameters\n",
    "n_epochs = 30 # Number of training epochs\n",
    "learning_rates = [1e-1, 1e-2, 1e-3] # Dynamic learning rate\n",
    "hidden_size = 10 # Number of LSTM cells\n",
    "dropout_rate = 0.0 # Dropout rate of the final fully connected Layer [0.0, 1.0]\n",
    "sequence_length = 365 # Length of the meteorological record provided to the network\n",
    "batch_size = 256\n",
    "\n",
    "# preload the weather dataframe\n",
    "# ========================================== preload forcing ==========================================\n",
    "def load_forcing(lake_id):\n",
    "    # load weather data\n",
    "    df_air = pd.read_csv(air_temp_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_wind = pd.read_csv(wind_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_srad = pd.read_csv(srad_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    # load weather data\n",
    "    weather_df = pd.concat([df_air, df_wind, df_srad], axis = 1)\n",
    "    weather_df.columns = [\"ta\", \"wind\", \"srad\"]\n",
    "    return weather_df\n",
    "\n",
    "# preload the water temperature\n",
    "def load_water_temp(lake_id):\n",
    "    # load water temperature\n",
    "    twdf = pd.read_csv(water_temp_path,\n",
    "                     index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)]).clip(0, 999)\n",
    "    twdf.columns = [\"tw\"]\n",
    "    return twdf.tw\n",
    "\n",
    "# lake dataframe\n",
    "total_df = load_forcing(lake_id)\n",
    "total_df['tw'] = load_water_temp(lake_id)\n",
    "\n",
    "@njit\n",
    "def reshape_data(x: np.ndarray, y: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reshape matrix data into sample shape for LSTM training.\n",
    "\n",
    "    :param x: Matrix containing input features column wise and time steps row wise\n",
    "    :param y: Matrix containing the output feature.\n",
    "    :param seq_length: Length of look back days for one day of prediction\n",
    "    \n",
    "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n",
    "        number of features), containing the input data for the LSTM. The second\n",
    "        of shape (samples, 1) containing the expected output for each input\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = x.shape\n",
    "\n",
    "    x_new = np.zeros((num_samples - seq_length + 1, seq_length, num_features))\n",
    "    y_new = np.zeros((num_samples - seq_length + 1, 1))\n",
    "\n",
    "    for i in range(0, x_new.shape[0]):\n",
    "        x_new[i, :, :num_features] = x[i:i + seq_length, :]\n",
    "        y_new[i, :] = y[i + seq_length - 1, 0]\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "class laketemp(Dataset):\n",
    "    def __init__(self, \n",
    "                 lake_id: str,\n",
    "                 total_df: pd.DataFrame=None, # combination of weather and obs water temperature\n",
    "                 seq_length: int=365,\n",
    "                 period: str=None,\n",
    "                 dates: List=None, \n",
    "                 means: pd.Series=None, \n",
    "                 stds: pd.Series=None):\n",
    "        self.lake_id = lake_id\n",
    "        self.seq_length = seq_length\n",
    "        self.period = period\n",
    "        self.dates = dates\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "        # load datafrmae\n",
    "        self.total_df = total_df\n",
    "        # load data into memory\n",
    "        self.x, self.y = self._load_data()\n",
    "\n",
    "        # store number of samples as class attribute\n",
    "        self.num_samples = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load input and output data from text files.\"\"\"\n",
    "        df = self.total_df\n",
    "        \n",
    "        if self.dates is not None:\n",
    "            # If meteorological observations exist before start date\n",
    "            # use these as well. Similiar to hydrological warmup period.\n",
    "            if self.dates[0] - pd.DateOffset(days=self.seq_length) > df.index[0]:\n",
    "                start_date = self.dates[0] - pd.DateOffset(days=self.seq_length)\n",
    "            else:\n",
    "                start_date = self.dates[0]\n",
    "            df = df[start_date:self.dates[1]]\n",
    "\n",
    "        # if training period store means and stds\n",
    "        if self.period == 'train':\n",
    "            self.means = df.mean()\n",
    "            self.stds = df.std()\n",
    "\n",
    "        # extract input and output features from DataFrame\n",
    "        x = np.array([df['ta'].values,\n",
    "                      df['wind'].values,\n",
    "                      df['srad'].values,\n",
    "                      ]).T\n",
    "        y = np.array([df['tw'].values]).T\n",
    "\n",
    "        # normalize data, reshape for LSTM training and remove invalid samples\n",
    "        x = self._local_normalization(x, variable='inputs')\n",
    "        x, y = reshape_data(x, y, self.seq_length)\n",
    "\n",
    "        if self.period == \"train\":\n",
    "            # Delete all samples, where discharge is NaN\n",
    "            if np.sum(np.isnan(y)) > 0:\n",
    "                print(f\"Deleted some records because of NaNs {self.lake_id}\")\n",
    "                x = np.delete(x, np.argwhere(np.isnan(y)), axis=0)\n",
    "                y = np.delete(y, np.argwhere(np.isnan(y)), axis=0)\n",
    "            \n",
    "            # Deletes all records, where no discharge was measured (-999)\n",
    "            x = np.delete(x, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            y = np.delete(y, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            \n",
    "            # normalize discharge\n",
    "            y = self._local_normalization(y, variable='output')\n",
    "\n",
    "        # convert arrays to torch tensors\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _local_normalization(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Normalize input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = (feature - means) / stds\n",
    "        elif variable == 'output':\n",
    "            feature = ((feature - self.means[\"tw\"]) /\n",
    "                       self.stds[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def local_rescale(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Rescale input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = feature * stds + means\n",
    "        elif variable == 'output':\n",
    "            feature = (feature * self.stds[\"tw\"] +\n",
    "                       self.means[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def get_means(self):\n",
    "        return self.means\n",
    "\n",
    "    def get_stds(self):\n",
    "        return self.stds\n",
    "    \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\"Implementation of a single layer LSTM network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, dropout_rate: float=0.0, normalized_zero: float = 0.0):\n",
    "        \"\"\"Initialize model\n",
    "        \n",
    "        :param hidden_size: Number of hidden units/LSTM cells\n",
    "        :param dropout_rate: Dropout rate of the last fully connected\n",
    "            layer. Default 0.0\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # create required layer\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=self.hidden_size, \n",
    "                            num_layers=1, bias=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=1)\n",
    "        self.normalized_zero = normalized_zero\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the Network.\n",
    "        \n",
    "        :param x: Tensor of shape [batch size, seq length, num features]\n",
    "            containing the input data for the LSTM network.\n",
    "        \n",
    "        :return: Tensor containing the network predictions\n",
    "        \"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # perform prediction only at the end of the input sequence\n",
    "        pred = self.fc(self.dropout(h_n[-1,:,:]))\n",
    "        \n",
    "        # clamp prediction of 0 C\n",
    "        pred = torch.clamp(pred, min = self.normalized_zero)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer, loader, loss_func, epoch):\n",
    "    \"\"\"Train model for a single epoch.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param optimizer: One of PyTorchs optimizer classes.\n",
    "    :param loader: A PyTorch DataLoader, providing the trainings\n",
    "        data in mini batches.\n",
    "    :param loss_func: The loss function to minimize.\n",
    "    :param epoch: The current epoch (int) used for the progress bar\n",
    "    \"\"\"\n",
    "    # set model to train mode (important for dropout)\n",
    "    model.train()\n",
    "    \n",
    "    # pbar = tqdm.tqdm_notebook(loader)\n",
    "    # pbar.set_description(f\"Epoch {epoch}\")\n",
    "    # request mini-batch of data from the loader\n",
    "    for xs, ys in loader:\n",
    "        # delete previously stored gradients from the model\n",
    "        optimizer.zero_grad()\n",
    "        # push data to GPU (if available)\n",
    "        xs, ys = xs.to(DEVICE), ys.to(DEVICE)\n",
    "        # get model predictions\n",
    "        y_hat = model(xs)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_hat, ys)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # write current loss in the progress bar\n",
    "        # pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "def eval_model(model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Evaluate the model.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param loader: A PyTorch DataLoader, providing the data.\n",
    "    \n",
    "    :return: Two torch Tensors, containing the observations and \n",
    "        model predictions\n",
    "    \"\"\"\n",
    "    # set model to eval mode (important for dropout)\n",
    "    model.eval()\n",
    "    obs = []\n",
    "    preds = []\n",
    "    # in inference mode, we don't need to store intermediate steps for\n",
    "    # backprob\n",
    "    with torch.no_grad():\n",
    "        # request mini-batch of data from the loader\n",
    "        for xs, ys in loader:\n",
    "            # push data to GPU (if available)\n",
    "            xs = xs.to(DEVICE)\n",
    "            # get model predictions\n",
    "            y_hat = model(xs)\n",
    "            obs.append(ys)\n",
    "            preds.append(y_hat)\n",
    "            \n",
    "    return torch.cat(obs), torch.cat(preds)\n",
    "        \n",
    "def calc_rmse(obs, sim):\n",
    "    \"\"\"Calculate the mean squared error.\n",
    "    Args:\n",
    "        obs: Array of the observed values\n",
    "        sim: Array of the simulated values\n",
    "    Returns:\n",
    "        The MSE value for the simulation, compared to the observation.\n",
    "    \"\"\"\n",
    "    # Validation check on the input arrays  \n",
    "    if len(obs) != len(sim):\n",
    "        raise ValueError(\"Arrays must have the same size.\")\n",
    "    # drop nan and negative temperature from the observation\n",
    "    nan_index = obs>=0  \n",
    "    obs = obs[nan_index]\n",
    "    sim = sim[nan_index]\n",
    "    \n",
    "    # Calculate the rmse value\n",
    "    rmse_val = np.sqrt(np.mean((obs-sim)**2))\n",
    "\n",
    "    return rmse_val\n",
    "\n",
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "\n",
    "    return nse_val\n",
    "\n",
    "def train_single(ensemble_id,\n",
    "                 total_df,\n",
    "                ):\n",
    "    ##############\n",
    "    # Data set up#\n",
    "    ##############\n",
    "    # Training data\n",
    "    ds_train = laketemp(lake_id, \n",
    "                        total_df,\n",
    "                        seq_length=sequence_length, \n",
    "                        period=\"train\", \n",
    "                        dates=train_period)\n",
    "    tr_loader = DataLoader(ds_train, \n",
    "                           batch_size=batch_size, \n",
    "                           shuffle=True)\n",
    "\n",
    "    # Validation data. We use the feature means/stds of the training period for normalization\n",
    "    means = ds_train.get_means()\n",
    "    stds = ds_train.get_stds()\n",
    "    ds_val = laketemp(lake_id, \n",
    "                      total_df,\n",
    "                      seq_length=sequence_length, \n",
    "                      period=\"eval\", \n",
    "                      dates=val_period,\n",
    "                      means=means, \n",
    "                      stds=stds)\n",
    "    \n",
    "    val_loader = DataLoader(ds_val, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)\n",
    "\n",
    "    # val_loader_cloud = DataLoader(ds_val_cloud, batch_size=2048, shuffle=False)\n",
    "\n",
    "    # Test data. We use the feature means/stds of the training period for normalization\n",
    "    # ds_test = laketemp(lake_id, \n",
    "    #                    total_df,\n",
    "    #                    seq_length=sequence_length, \n",
    "    #                    period=\"eval\", dates=test_period,\n",
    "    #                    means=means, stds=stds)\n",
    "    # test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    #########################\n",
    "    # Model, Optimizer, Loss\n",
    "    #########################\n",
    "\n",
    "    # based on the mean and stds, calculate the normalized value for 0 C\n",
    "    normalized_zero = (0 - means[\"tw\"])/stds[\"tw\"]\n",
    "\n",
    "\n",
    "    # Here we create our model, feel free \n",
    "    model = Model(hidden_size=hidden_size, dropout_rate=dropout_rate, normalized_zero = normalized_zero).to(DEVICE)\n",
    "    loss_func = nn.MSELoss()\n",
    "    \n",
    "    for i in range(n_epochs):\n",
    "        if i < 10:\n",
    "            learning_rate = learning_rates[0]\n",
    "        elif 10 < i <= 20:\n",
    "            learning_rate = learning_rates[1]\n",
    "        elif 20 < i <= 30:\n",
    "            learning_rate = learning_rates[2]\n",
    "        else:\n",
    "            learning_rate = learning_rates[-1]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        train_epoch(model, optimizer, tr_loader, loss_func, i+1)\n",
    "        obs, preds = eval_model(model, val_loader)\n",
    "        preds = ds_val.local_rescale(preds.cpu().numpy(), variable='output')\n",
    "        rmse = calc_rmse(obs.numpy(), preds)\n",
    "        nse = calc_nse(obs.numpy(), preds)\n",
    "    print(f\"Validation RMSE: {rmse:.2f}, NSE: {nse:.2f}\")\n",
    "    # save the model parameters\n",
    "    torch.save(model.state_dict(), f\"{param_dir}/{lake_id}_{ensemble_id}.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for ensemble_id in range(ensemble_num):\n",
    "        # set manual seed\n",
    "        torch.manual_seed(ensemble_id)\n",
    "        train_single(ensemble_id, total_df)\n",
    "        print(f\"Ensemble {ensemble_id} done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
