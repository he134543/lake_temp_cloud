{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fe7c2d2-b542-401a-b2b4-45c2b8f4736e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "from tools.metrics import calc_nse, calc_kge\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce69279-a281-4188-ab18-c7af3d7848af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "lake_id_list = pd.read_csv(\"data/cci_lakes_hydrolake_depth.csv\")[\"CCI ID\"].to_numpy()\n",
    "param_dir = \"/nas/cee-hydro/laketemp_bias/params/lstm_param_cloud\"\n",
    "sim_dir = \"/nas/cee-hydro/laketemp_bias/simulations/lstm_cloud_sim\"\n",
    "air_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/air_temp.csv\"\n",
    "wind_path = \"/nas/cee-hydro/laketemp_bias/era5land/wind.csv\"\n",
    "srad_path = \"/nas/cee-hydro/laketemp_bias/era5land/srad.csv\"\n",
    "# water_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/water_temp.csv\"\n",
    "water_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/water_temp_cloud_25.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d06ab9-7f59-4b21-b27a-15afedccc31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     3350,       562,      1526, ..., 300000771,       276,\n",
       "       300004882])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_lakes = np.array([os.path.basename(fp).replace(\".csv\", \"\") for fp in glob.glob(f\"{sim_dir}/*.csv\")]\n",
    "                         ).astype(int)\n",
    "finished_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3787bc9c-6611-4956-8073-644b938a36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_lakes = np.setdiff1d(lake_id_list, finished_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2571d19b-7830-4f39-af9a-68c2e24e1bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      121,       166,       198,       277,       320,       321,\n",
       "             378,       524,       551,       756,      1130,      1321,\n",
       "            1913,      2168,      2731, 300007748, 300008201, 300012234,\n",
       "       300014185, 300015267, 300016309])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfinished_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d45b0b-ee18-4a3c-8150-c293de6a755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57f357-e7a9-4b34-b073-50102f93b68b",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e3d765-5ef4-4383-8502-9b3193d38889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_forcing(lake_id):\n",
    "    # load weather data\n",
    "    df_air = pd.read_csv(air_temp_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_wind = pd.read_csv(wind_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_srad = pd.read_csv(srad_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    # load weather data\n",
    "    weather_df = pd.concat([df_air, df_wind, df_srad], axis = 1)\n",
    "    weather_df.columns = [\"ta\", \"wind\", \"srad\"]\n",
    "    return weather_df\n",
    "\n",
    "def load_water_temp(lake_id):\n",
    "    # load water temperature\n",
    "    df = pd.read_csv(water_temp_path,\n",
    "                     index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)]).clip(0, 999)\n",
    "    df.columns = [\"tw\"]\n",
    "    return df.tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005491c1-5ddc-4149-92b8-181fc3268661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def reshape_data(x: np.ndarray, y: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reshape matrix data into sample shape for LSTM training.\n",
    "\n",
    "    :param x: Matrix containing input features column wise and time steps row wise\n",
    "    :param y: Matrix containing the output feature.\n",
    "    :param seq_length: Length of look back days for one day of prediction\n",
    "    \n",
    "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n",
    "        number of features), containing the input data for the LSTM. The second\n",
    "        of shape (samples, 1) containing the expected output for each input\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = x.shape\n",
    "\n",
    "    x_new = np.zeros((num_samples - seq_length + 1, seq_length, num_features))\n",
    "    y_new = np.zeros((num_samples - seq_length + 1, 1))\n",
    "\n",
    "    for i in range(0, x_new.shape[0]):\n",
    "        x_new[i, :, :num_features] = x[i:i + seq_length, :]\n",
    "        y_new[i, :] = y[i + seq_length - 1, 0]\n",
    "\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8766a8-5228-43f3-97a9-afdef81385fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class laketemp(Dataset):\n",
    "    def __init__(self, \n",
    "                 lake_id: str, \n",
    "                 seq_length: int=365,\n",
    "                 period: str=None,\n",
    "                 dates: List=None, \n",
    "                 means: pd.Series=None, \n",
    "                 stds: pd.Series=None):\n",
    "        self.lake_id = lake_id\n",
    "        self.seq_length = seq_length\n",
    "        self.period = period\n",
    "        self.dates = dates\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "        # load data into memory\n",
    "        self.x, self.y = self._load_data()\n",
    "\n",
    "        # store number of samples as class attribute\n",
    "        self.num_samples = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load input and output data from text files.\"\"\"\n",
    "        df = load_forcing(self.lake_id)\n",
    "        df['tw'] = load_water_temp(self.lake_id).to_numpy()\n",
    "        \n",
    "        if self.dates is not None:\n",
    "            # If meteorological observations exist before start date\n",
    "            # use these as well. Similiar to hydrological warmup period.\n",
    "            if self.dates[0] - pd.DateOffset(days=self.seq_length) > df.index[0]:\n",
    "                start_date = self.dates[0] - pd.DateOffset(days=self.seq_length)\n",
    "            else:\n",
    "                start_date = self.dates[0]\n",
    "            df = df[start_date:self.dates[1]]\n",
    "\n",
    "        # if training period store means and stds\n",
    "        if self.period == 'train':\n",
    "            self.means = df.mean()\n",
    "            self.stds = df.std()\n",
    "\n",
    "        # extract input and output features from DataFrame\n",
    "        x = np.array([df['ta'].values,\n",
    "                      df['wind'].values,\n",
    "                      df['srad'].values,\n",
    "                      ]).T\n",
    "        y = np.array([df['tw'].values]).T\n",
    "\n",
    "        # normalize data, reshape for LSTM training and remove invalid samples\n",
    "        x = self._local_normalization(x, variable='inputs')\n",
    "        x, y = reshape_data(x, y, self.seq_length)\n",
    "\n",
    "        if self.period == \"train\":\n",
    "            # Delete all samples, where discharge is NaN\n",
    "            if np.sum(np.isnan(y)) > 0:\n",
    "                print(f\"Deleted some records because of NaNs {self.lake_id}\")\n",
    "                x = np.delete(x, np.argwhere(np.isnan(y)), axis=0)\n",
    "                y = np.delete(y, np.argwhere(np.isnan(y)), axis=0)\n",
    "            \n",
    "            # Deletes all records, where no discharge was measured (-999)\n",
    "            x = np.delete(x, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            y = np.delete(y, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            \n",
    "            # normalize discharge\n",
    "            y = self._local_normalization(y, variable='output')\n",
    "\n",
    "        # convert arrays to torch tensors\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _local_normalization(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Normalize input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = (feature - means) / stds\n",
    "        elif variable == 'output':\n",
    "            feature = ((feature - self.means[\"tw\"]) /\n",
    "                       self.stds[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def local_rescale(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Rescale input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = feature * stds + means\n",
    "        elif variable == 'output':\n",
    "            feature = (feature * self.stds[\"tw\"] +\n",
    "                       self.means[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def get_means(self):\n",
    "        return self.means\n",
    "\n",
    "    def get_stds(self):\n",
    "        return self.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70521e7-4b83-41f5-a75b-bcbf497cdf00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Implementation of a single layer LSTM network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, dropout_rate: float=0.0):\n",
    "        \"\"\"Initialize model\n",
    "        \n",
    "        :param hidden_size: Number of hidden units/LSTM cells\n",
    "        :param dropout_rate: Dropout rate of the last fully connected\n",
    "            layer. Default 0.0\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # create required layer\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=self.hidden_size, \n",
    "                            num_layers=1, bias=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the Network.\n",
    "        \n",
    "        :param x: Tensor of shape [batch size, seq length, num features]\n",
    "            containing the input data for the LSTM network.\n",
    "        \n",
    "        :return: Tensor containing the network predictions\n",
    "        \"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # perform prediction only at the end of the input sequence\n",
    "        pred = self.fc(self.dropout(h_n[-1,:,:]))\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d1639d-360d-44f8-99ec-80d4a994eb22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, loss_func, epoch):\n",
    "    \"\"\"Train model for a single epoch.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param optimizer: One of PyTorchs optimizer classes.\n",
    "    :param loader: A PyTorch DataLoader, providing the trainings\n",
    "        data in mini batches.\n",
    "    :param loss_func: The loss function to minimize.\n",
    "    :param epoch: The current epoch (int) used for the progress bar\n",
    "    \"\"\"\n",
    "    # set model to train mode (important for dropout)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm_notebook(loader)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    # request mini-batch of data from the loader\n",
    "    for xs, ys in pbar:\n",
    "        # delete previously stored gradients from the model\n",
    "        optimizer.zero_grad()\n",
    "        # push data to GPU (if available)\n",
    "        xs, ys = xs.to(DEVICE), ys.to(DEVICE)\n",
    "        # get model predictions\n",
    "        y_hat = model(xs)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_hat, ys)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # write current loss in the progress bar\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "def eval_model(model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Evaluate the model.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param loader: A PyTorch DataLoader, providing the data.\n",
    "    \n",
    "    :return: Two torch Tensors, containing the observations and \n",
    "        model predictions\n",
    "    \"\"\"\n",
    "    # set model to eval mode (important for dropout)\n",
    "    model.eval()\n",
    "    obs = []\n",
    "    preds = []\n",
    "    # in inference mode, we don't need to store intermediate steps for\n",
    "    # backprob\n",
    "    with torch.no_grad():\n",
    "        # request mini-batch of data from the loader\n",
    "        for xs, ys in loader:\n",
    "            # push data to GPU (if available)\n",
    "            xs = xs.to(DEVICE)\n",
    "            # get model predictions\n",
    "            y_hat = model(xs)\n",
    "            obs.append(ys)\n",
    "            preds.append(y_hat)\n",
    "            \n",
    "    return torch.cat(obs), torch.cat(preds)\n",
    "        \n",
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # only consider time steps, where observations are available\n",
    "    sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n",
    "\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "\n",
    "    return nse_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0991fa4-e5ae-4d1d-8f0e-36ff73df7f3f",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c70c634-0ca4-47a5-b2b7-7025c209e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(lake_id,\n",
    "             sim_dir = sim_dir,\n",
    "            simulation_period = [pd.to_datetime(\"2001-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\")], # need 365 days for delay\n",
    "            hidden_size = 10, # Number of LSTM cells\n",
    "            dropout_rate = 0.0, # Dropout rate of the final fully connected Layer [0.0, 1.0]\n",
    "            learning_rate = 1e-3, # Learning rate used to update the weights\n",
    "            sequence_length = 365, # Length of the meteorological record provided to the network\n",
    "            batch_size = 2048, # batch size for simulation \n",
    "            train_period = [pd.to_datetime(\"2000-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2014-12-31\", format=\"%Y-%m-%d\")],\n",
    "            ):\n",
    "    # create a train set to get the mean and stds\n",
    "    ds_train = laketemp(lake_id, seq_length=sequence_length, period=\"train\", dates=train_period)\n",
    "    means = ds_train.get_means()\n",
    "    stds = ds_train.get_stds()\n",
    "    # simulate, use the 'eval' dataloader\n",
    "    ds_sim = laketemp(lake_id, seq_length=sequence_length, period=\"eval\", dates=simulation_period,\n",
    "                     means=means, stds=stds)\n",
    "    sim_loader = DataLoader(ds_sim, batch_size=batch_size, shuffle=False)\n",
    "    #########################\n",
    "    # Model, Optimizer, Loss#\n",
    "    #########################\n",
    "\n",
    "    # Here we create our model, feel free \n",
    "    model = Model(hidden_size=hidden_size, dropout_rate=dropout_rate).to(DEVICE)\n",
    "    # load parameter\n",
    "    param_path = f\"{param_dir}/{lake_id}.pt\"\n",
    "    model.load_state_dict(torch.load(param_path))\n",
    "    model.eval()\n",
    "    # predict\n",
    "    obs, preds = eval_model(model, sim_loader)\n",
    "    # rescale prediction\n",
    "    preds = ds_sim.local_rescale(preds.cpu().numpy(), variable='output')\n",
    "    # create a dataframe\n",
    "    start_date = ds_sim.dates[0]\n",
    "    end_date = ds_sim.dates[1] + pd.DateOffset(days=1)    \n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "    output_df = pd.DataFrame(preds, index = date_range)\n",
    "    output_df.columns = [\"tw\"]\n",
    "    # save to\n",
    "    output_df.to_csv(f\"{sim_dir}/{lake_id}.csv\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9afd6a-3244-4a9c-b4f5-d84e4f18d929",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/21 [00:38<12:44, 38.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:49<07:08, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [01:01<05:16, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [01:12<04:17, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [01:24<03:40, 13.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [01:35<03:14, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [01:46<02:53, 12.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8/21 [01:58<02:37, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9/21 [02:09<02:23, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [02:21<02:09, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [02:32<01:56, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [02:43<01:43, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 1913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13/21 [02:55<01:31, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 2168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [03:06<01:19, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 2731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15/21 [03:17<01:08, 11.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300007748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16/21 [03:29<00:57, 11.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300008201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [03:40<00:45, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300012234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 18/21 [03:51<00:34, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300014185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19/21 [04:03<00:22, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300015267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [04:14<00:11, 11.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 300016309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:25<00:00, 12.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for cci_lake_id in tqdm.tqdm(unfinished_lakes):\n",
    "    simulate(cci_lake_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
