{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee40d52f-90f6-449f-bf8c-fb2e2bfb56d6",
   "metadata": {},
   "source": [
    "- Hyperparameter searching with 5-fold cross validation\n",
    "- Train with the best hyperparameters\n",
    "- Simulate with the trained models (10 ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608de73-f79e-4534-b4a5-c94c06040bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 2] Early stopping at epoch 72\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "Params {'hidden_size': 8, 'dropout_rate': 0.0} => Avg. RMSE: 0.34\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 1] Early stopping at epoch 59\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 2] Early stopping at epoch 90\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 3] Early stopping at epoch 62\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 4] Early stopping at epoch 68\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 5] Early stopping at epoch 38\n",
      "Params {'hidden_size': 8, 'dropout_rate': 0.1} => Avg. RMSE: 0.46\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 1] Early stopping at epoch 65\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 2] Early stopping at epoch 79\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 3] Early stopping at epoch 59\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 4] Early stopping at epoch 42\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 5] Early stopping at epoch 78\n",
      "Params {'hidden_size': 8, 'dropout_rate': 0.2} => Avg. RMSE: 0.47\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 4] Early stopping at epoch 99\n",
      "Deleted some records because of NaNs 733\n",
      "Params {'hidden_size': 16, 'dropout_rate': 0.0} => Avg. RMSE: 0.27\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 2] Early stopping at epoch 97\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 3] Early stopping at epoch 86\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 4] Early stopping at epoch 93\n",
      "Deleted some records because of NaNs 733\n",
      "Params {'hidden_size': 16, 'dropout_rate': 0.1} => Avg. RMSE: 0.33\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 1] Early stopping at epoch 61\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 2] Early stopping at epoch 40\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 3] Early stopping at epoch 15\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 4] Early stopping at epoch 33\n",
      "Deleted some records because of NaNs 733\n",
      "[Fold 5] Early stopping at epoch 51\n",
      "Params {'hidden_size': 16, 'dropout_rate': 0.2} => Avg. RMSE: 0.66\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n",
      "Deleted some records because of NaNs 733\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "\n",
    "os.chdir(\"/work/pi_kandread_umass_edu/lake_temp_bias/satbias_model/satlswt\")\n",
    "\n",
    "# ========================================= Training characteristics ==============================================\n",
    "lake_id = str(733)\n",
    "\n",
    "# file directory to save parameter\n",
    "# param_dir = f\"/work/pi_kandread_umass_edu/lake_temp_bias/satbias_model/satlswt/params/lstm_param_{model}\"\n",
    "param_dir = f\"/nas/cee-hydro/laketemp_bias/params/lstm_param_cloud\"\n",
    "sim_dir = f\"/nas/cee-hydro/laketemp_bias/simulations/lstm_cloud_sim\"\n",
    "\n",
    "# data path\n",
    "air_temp_path = \"/nas/cee-hydro/laketemp_bias/era5land/air_temp.csv\"\n",
    "wind_path = \"/nas/cee-hydro/laketemp_bias/era5land/wind.csv\"\n",
    "srad_path = \"/nas/cee-hydro/laketemp_bias/era5land/srad.csv\"\n",
    "water_temp_path = f\"/nas/cee-hydro/laketemp_bias/era5land/water_temp_cloud.csv\"\n",
    "\n",
    "# ensemble\n",
    "ensemble_num = 10\n",
    "# use cpu or gpu\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# train test period\n",
    "train_period = (pd.to_datetime(\"2003-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2017-12-31\", format=\"%Y-%m-%d\"))\n",
    "val_period = (pd.to_datetime(\"2018-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\"))\n",
    "sim_period = (pd.to_datetime(\"2001-01-01\", format=\"%Y-%m-%d\"), # needs 365 days in advance\n",
    "              pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\"))\n",
    "# test_period = (pd.to_datetime(\"2020-01-01\", format=\"%Y-%m-%d\"), pd.to_datetime(\"2023-12-31\", format=\"%Y-%m-%d\"))\n",
    "\n",
    "# hyperparameters\n",
    "n_epochs = 100 # Number of training epochs\n",
    "initial_lr = 1e-1  # or 1e-3 depending on how aggressive you want to start\n",
    "decay_rate = 0.96  # decay per epoch\n",
    "sequence_length = 365 # Length of the meteorological record provided to the network\n",
    "batch_size = 256\n",
    "\n",
    "# hyper parameters needs to be turned\n",
    "hidden_size_list = [8, 16, 32] # Number of hidden layer units\n",
    "dropout_rate = [0.0, 0.1, 0.2] # Dropout rate of the final fully connected Layer [0.0, 1.0]\n",
    "\n",
    "# preload the weather dataframe\n",
    "# ========================================== preload forcing ==========================================\n",
    "def load_forcing(lake_id):\n",
    "    # load weather data\n",
    "    df_air = pd.read_csv(air_temp_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_wind = pd.read_csv(wind_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    df_srad = pd.read_csv(srad_path, index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)])\n",
    "    # load weather data\n",
    "    weather_df = pd.concat([df_air, df_wind, df_srad], axis = 1)\n",
    "    weather_df.columns = [\"ta\", \"wind\", \"srad\"]\n",
    "    return weather_df\n",
    "\n",
    "# preload the water temperature\n",
    "def load_water_temp(lake_id):\n",
    "    # load water temperature\n",
    "    twdf = pd.read_csv(water_temp_path,\n",
    "                     index_col=0, parse_dates=True, usecols=[\"Unnamed: 0\", str(lake_id)]).clip(0, 999)\n",
    "    twdf.columns = [\"tw\"]\n",
    "    return twdf.tw\n",
    "\n",
    "# lake dataframe\n",
    "total_df = load_forcing(lake_id)\n",
    "total_df['tw'] = load_water_temp(lake_id)\n",
    "\n",
    "@njit\n",
    "def reshape_data(x: np.ndarray, y: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reshape matrix data into sample shape for LSTM training.\n",
    "\n",
    "    :param x: Matrix containing input features column wise and time steps row wise\n",
    "    :param y: Matrix containing the output feature.\n",
    "    :param seq_length: Length of look back days for one day of prediction\n",
    "    \n",
    "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n",
    "        number of features), containing the input data for the LSTM. The second\n",
    "        of shape (samples, 1) containing the expected output for each input\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = x.shape\n",
    "\n",
    "    x_new = np.zeros((num_samples - seq_length + 1, seq_length, num_features))\n",
    "    y_new = np.zeros((num_samples - seq_length + 1, 1))\n",
    "\n",
    "    for i in range(0, x_new.shape[0]):\n",
    "        x_new[i, :, :num_features] = x[i:i + seq_length, :]\n",
    "        y_new[i, :] = y[i + seq_length - 1, 0]\n",
    "\n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "class laketemp(Dataset):\n",
    "    def __init__(self, \n",
    "                 lake_id: str,\n",
    "                 total_df: pd.DataFrame=None, # combination of weather and obs water temperature\n",
    "                 seq_length: int=365,\n",
    "                 period: str=None,\n",
    "                 dates: List=None, \n",
    "                 means: pd.Series=None, \n",
    "                 stds: pd.Series=None):\n",
    "        self.lake_id = lake_id\n",
    "        self.seq_length = seq_length\n",
    "        self.period = period\n",
    "        self.dates = dates\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "        # load datafrmae\n",
    "        self.total_df = total_df\n",
    "        # load data into memory\n",
    "        self.x, self.y = self._load_data()\n",
    "\n",
    "        # store number of samples as class attribute\n",
    "        self.num_samples = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load input and output data from text files.\"\"\"\n",
    "        df = self.total_df\n",
    "        \n",
    "        if self.dates is not None:\n",
    "            # If meteorological observations exist before start date\n",
    "            # use these as well. Similiar to hydrological warmup period.\n",
    "            if self.dates[0] - pd.DateOffset(days=self.seq_length) > df.index[0]:\n",
    "                start_date = self.dates[0] - pd.DateOffset(days=self.seq_length)\n",
    "            else:\n",
    "                start_date = self.dates[0]\n",
    "            df = df[start_date:self.dates[1]]\n",
    "\n",
    "        # if training period store means and stds\n",
    "        if self.period == 'train':\n",
    "            self.means = df.mean()\n",
    "            self.stds = df.std()\n",
    "\n",
    "        # extract input and output features from DataFrame\n",
    "        x = np.array([df['ta'].values,\n",
    "                      df['wind'].values,\n",
    "                      df['srad'].values,\n",
    "                      ]).T\n",
    "        y = np.array([df['tw'].values]).T\n",
    "\n",
    "        # normalize data, reshape for LSTM training and remove invalid samples\n",
    "        x = self._local_normalization(x, variable='inputs')\n",
    "        x, y = reshape_data(x, y, self.seq_length)\n",
    "\n",
    "        if self.period == \"train\":\n",
    "            # Delete all samples, where discharge is NaN\n",
    "            if np.sum(np.isnan(y)) > 0:\n",
    "                print(f\"Deleted some records because of NaNs {self.lake_id}\")\n",
    "                x = np.delete(x, np.argwhere(np.isnan(y)), axis=0)\n",
    "                y = np.delete(y, np.argwhere(np.isnan(y)), axis=0)\n",
    "            \n",
    "            # Deletes all records, where no discharge was measured (-999)\n",
    "            x = np.delete(x, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            y = np.delete(y, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            \n",
    "            # normalize discharge\n",
    "            y = self._local_normalization(y, variable='output')\n",
    "\n",
    "        # convert arrays to torch tensors\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _local_normalization(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Normalize input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = (feature - means) / stds\n",
    "        elif variable == 'output':\n",
    "            feature = ((feature - self.means[\"tw\"]) /\n",
    "                       self.stds[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def local_rescale(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Rescale input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['ta'],\n",
    "                              self.means['wind'],\n",
    "                              self.means['srad']])\n",
    "            stds = np.array([self.stds['ta'],\n",
    "                             self.stds['wind'],\n",
    "                             self.stds['srad']])\n",
    "            feature = feature * stds + means\n",
    "        elif variable == 'output':\n",
    "            feature = (feature * self.stds[\"tw\"] +\n",
    "                       self.means[\"tw\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def get_means(self):\n",
    "        return self.means\n",
    "\n",
    "    def get_stds(self):\n",
    "        return self.stds\n",
    "    \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\"Implementation of a single layer LSTM network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, dropout_rate: float=0.0, normalized_zero: float = 0.0):\n",
    "        \"\"\"Initialize model\n",
    "        \n",
    "        :param hidden_size: Number of hidden units/LSTM cells\n",
    "        :param dropout_rate: Dropout rate of the last fully connected\n",
    "            layer. Default 0.0\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # create required layer\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=self.hidden_size, \n",
    "                            num_layers=1, bias=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=1)\n",
    "        self.normalized_zero = normalized_zero\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the Network.\n",
    "        \n",
    "        :param x: Tensor of shape [batch size, seq length, num features]\n",
    "            containing the input data for the LSTM network.\n",
    "        \n",
    "        :return: Tensor containing the network predictions\n",
    "        \"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # perform prediction only at the end of the input sequence\n",
    "        pred = self.fc(self.dropout(h_n[-1,:,:]))\n",
    "        \n",
    "        # clamp prediction of 0 C\n",
    "        # pred = torch.clamp(pred, min = self.normalized_zero)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    \n",
    "def train_epoch(model, optimizer, loader, loss_func, epoch):\n",
    "    \"\"\"Train model for a single epoch.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param optimizer: One of PyTorchs optimizer classes.\n",
    "    :param loader: A PyTorch DataLoader, providing the trainings\n",
    "        data in mini batches.\n",
    "    :param loss_func: The loss function to minimize.\n",
    "    :param epoch: The current epoch (int) used for the progress bar\n",
    "    \"\"\"\n",
    "    # set model to train mode (important for dropout)\n",
    "    model.train()\n",
    "    \n",
    "    # pbar = tqdm.tqdm_notebook(loader)\n",
    "    # pbar.set_description(f\"Epoch {epoch}\")\n",
    "    # request mini-batch of data from the loader\n",
    "    for xs, ys in loader:\n",
    "        # delete previously stored gradients from the model\n",
    "        optimizer.zero_grad()\n",
    "        # push data to GPU (if available)\n",
    "        xs, ys = xs.to(DEVICE), ys.to(DEVICE)\n",
    "        # get model predictions\n",
    "        y_hat = model(xs)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_hat, ys)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # write current loss in the progress bar\n",
    "        # pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "def eval_model(model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Evaluate the model.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param loader: A PyTorch DataLoader, providing the data.\n",
    "    \n",
    "    :return: Two torch Tensors, containing the observations and \n",
    "        model predictions\n",
    "    \"\"\"\n",
    "    # set model to eval mode (important for dropout)\n",
    "    model.eval()\n",
    "    obs = []\n",
    "    preds = []\n",
    "    # in inference mode, we don't need to store intermediate steps for\n",
    "    # backprob\n",
    "    with torch.no_grad():\n",
    "        # request mini-batch of data from the loader\n",
    "        for xs, ys in loader:\n",
    "            # push data to GPU (if available)\n",
    "            xs = xs.to(DEVICE)\n",
    "            # get model predictions\n",
    "            y_hat = model(xs)\n",
    "            obs.append(ys)\n",
    "            preds.append(y_hat)\n",
    "            \n",
    "    return torch.cat(obs), torch.cat(preds)\n",
    "        \n",
    "def calc_rmse(obs, sim):\n",
    "    \"\"\"Calculate the mean squared error.\n",
    "    Args:\n",
    "        obs: Array of the observed values\n",
    "        sim: Array of the simulated values\n",
    "    Returns:\n",
    "        The MSE value for the simulation, compared to the observation.\n",
    "    \"\"\"\n",
    "    # Validation check on the input arrays  \n",
    "    if len(obs) != len(sim):\n",
    "        raise ValueError(\"Arrays must have the same size.\")\n",
    "    # drop nan and negative temperature from the observation\n",
    "    nan_index = obs>=0  \n",
    "    obs = obs[nan_index]\n",
    "    sim = sim[nan_index]\n",
    "    \n",
    "    # Calculate the rmse value\n",
    "    rmse_val = np.sqrt(np.mean((obs-sim)**2))\n",
    "\n",
    "    return rmse_val\n",
    "\n",
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "\n",
    "    return nse_val\n",
    "\n",
    "def run_kfold_cv(hparams, total_df, k=5, patience=10):\n",
    "    rmse_scores = []\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    all_dates = total_df.index[(total_df.index >= train_period[0]) & (total_df.index <= train_period[1])]\n",
    "    all_dates = sorted(all_dates)[sequence_length:]\n",
    "\n",
    "    date_splits = list(kf.split(all_dates))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(date_splits):\n",
    "        train_dates = (all_dates[train_idx[0]], all_dates[train_idx[-1]])\n",
    "        val_dates = (all_dates[val_idx[0]], all_dates[val_idx[-1]])\n",
    "\n",
    "        ds_train = laketemp(lake_id, total_df, seq_length=sequence_length, period=\"train\", dates=train_dates)\n",
    "        tr_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        ds_val = laketemp(lake_id, total_df, seq_length=sequence_length, period=\"eval\", dates=val_dates,\n",
    "                          means=ds_train.get_means(), stds=ds_train.get_stds())\n",
    "        val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        normalized_zero = (0 - ds_train.get_means()[\"tw\"]) / ds_train.get_stds()[\"tw\"]\n",
    "        model = Model(hidden_size=hparams[\"hidden_size\"],\n",
    "                      dropout_rate=hparams[\"dropout_rate\"],\n",
    "                      normalized_zero=normalized_zero).to(DEVICE)\n",
    "        loss_func = nn.MSELoss()\n",
    "\n",
    "        best_rmse = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            lr = initial_lr * (decay_rate ** epoch)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            train_epoch(model, optimizer, tr_loader, loss_func, epoch+1)\n",
    "\n",
    "            obs, preds = eval_model(model, val_loader)\n",
    "            preds = ds_val.local_rescale(preds.cpu().numpy(), variable='output')\n",
    "            rmse = calc_rmse(obs.numpy(), preds)\n",
    "\n",
    "            if rmse < best_rmse - 1e-3:\n",
    "                best_rmse = rmse\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"[Fold {fold+1}] Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        rmse_scores.append(best_rmse)\n",
    "\n",
    "    return np.mean(rmse_scores)\n",
    "\n",
    "\n",
    "def select_best_hyperparams(total_df):\n",
    "    param_grid = list(product(hidden_size_list, dropout_rate))\n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for hs, dr in param_grid:\n",
    "        hparams = {\"hidden_size\": hs, \"dropout_rate\": dr}\n",
    "        avg_rmse = run_kfold_cv(hparams, total_df, k=5)\n",
    "        print(f\"Params {hparams} => Avg. RMSE: {avg_rmse:.2f}\")\n",
    "        if avg_rmse < best_score:\n",
    "            best_score = avg_rmse\n",
    "            best_params = hparams\n",
    "\n",
    "    print(f\"\\nBest Hyperparameters: {best_params}, RMSE: {best_score:.2f}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def train_and_simulate_best_params(ensemble_id, total_df, best_params, patience=10):\n",
    "    ds_train = laketemp(lake_id, total_df, seq_length=sequence_length, period=\"train\", dates=train_period)\n",
    "    tr_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    ds_val = laketemp(lake_id, total_df, seq_length=sequence_length, period=\"eval\", dates=val_period,\n",
    "                      means=ds_train.get_means(), stds=ds_train.get_stds())\n",
    "    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    normalized_zero = (0 - ds_train.get_means()[\"tw\"]) / ds_train.get_stds()[\"tw\"]\n",
    "    model = Model(hidden_size=best_params[\"hidden_size\"],\n",
    "                  dropout_rate=best_params[\"dropout_rate\"],\n",
    "                  normalized_zero=normalized_zero).to(DEVICE)\n",
    "\n",
    "    loss_func = nn.MSELoss()\n",
    "    best_rmse = float('inf')\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        lr = initial_lr * (decay_rate ** epoch)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        train_epoch(model, optimizer, tr_loader, loss_func, epoch+1)\n",
    "\n",
    "        obs, preds = eval_model(model, val_loader)\n",
    "        preds = ds_val.local_rescale(preds.cpu().numpy(), variable='output')\n",
    "        rmse = calc_rmse(obs.numpy(), preds)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] RMSE: {rmse:.2f}\")\n",
    "\n",
    "        if rmse < best_rmse - 1e-3:  # Small threshold to consider as \"improvement\"\n",
    "            best_rmse = rmse\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Load best model state and save\n",
    "    model.load_state_dict(best_state)\n",
    "    obs, preds = eval_model(model, val_loader)\n",
    "    preds = ds_val.local_rescale(preds.cpu().numpy(), variable='output')\n",
    "    final_rmse = calc_rmse(obs.numpy(), preds)\n",
    "    final_nse = calc_nse(obs.numpy(), preds)\n",
    "    param_str = f\"hs{best_params['hidden_size']}_dr{int(best_params['dropout_rate']*100)}\"\n",
    "    torch.save(model.state_dict(), f\"{param_dir}/{lake_id}_{ensemble_id}_{param_str}.pt\")\n",
    "    \n",
    "    \n",
    "    # --- Predict entire time series over sim_period ---\n",
    "    ds_total = laketemp(lake_id, total_df, seq_length=sequence_length, period=\"eval\",\n",
    "                        dates=sim_period,\n",
    "                        means=ds_train.get_means(), stds=ds_train.get_stds())\n",
    "    total_loader = DataLoader(ds_total, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    obs_total, preds_total = eval_model(model, total_loader)\n",
    "    preds_total = ds_total.local_rescale(preds_total.cpu().numpy(), variable='output')\n",
    "    obs_total = obs_total.numpy()\n",
    "\n",
    "    # Get date index for reshaped outputs\n",
    "    index = pd.date_range(start=sim_period[0],\n",
    "                          end=sim_period[1] + pd.DateOffset(days=1), freq='D')\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        f'sim_{ensemble_id}': preds_total.flatten()\n",
    "    }, index=index)\n",
    "\n",
    "    return result_df  # <--- return the predictions\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_params = select_best_hyperparams(total_df)\n",
    "    all_preds = []\n",
    "    for ensemble_id in range(ensemble_num):\n",
    "        torch.manual_seed(ensemble_id)\n",
    "        pred_df = train_and_simulate_best_params(ensemble_id, total_df, best_params)\n",
    "        all_preds.append(pred_df)\n",
    "        print(f\"Ensemble {ensemble_id} done\")\n",
    "\n",
    "    # Merge all ensemble predictions\n",
    "    ensemble_preds = pd.concat(all_preds, axis=1)\n",
    "\n",
    "    # Save full dataframe\n",
    "    out_csv = f\"{sim_dir}/{lake_id}.csv\"\n",
    "    ensemble_preds.to_csv(out_csv)\n",
    "    print(f\"Saved ensemble predictions to: {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c409d-b527-43c8-84a6-b7566223cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pytorch]",
   "language": "python",
   "name": "conda-env-.conda-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
